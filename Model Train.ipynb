{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649509b-2831-40a0-9e80-8340650731da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "CSV_PATH = \"translit_mistakes.csv\"\n",
    "MODEL_SAVE_PATH = \"translit_model.pt\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPOCHS = 5\n",
    "MAX_LEN = 32\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "PAD_TOKEN = \"<pad>\"\n",
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "df[\"MISTAKE\"] = df[\"MISTAKE\"].astype(str)\n",
    "df[\"CORRECT\"] = df[\"CORRECT\"].astype(str)\n",
    "\n",
    "\n",
    "all_text = list(df[\"MISTAKE\"]) + list(df[\"CORRECT\"])\n",
    "unique_chars = set()\n",
    "for word in all_text:\n",
    "    for ch in word:\n",
    "        unique_chars.add(ch)\n",
    "\n",
    "\n",
    "vocab_list = [PAD_TOKEN, BOS_TOKEN, EOS_TOKEN] + sorted(list(unique_chars))\n",
    "vocab2idx = {v: i for i, v in enumerate(vocab_list)}\n",
    "idx2vocab = {i: v for i, v in enumerate(vocab_list)}\n",
    "\n",
    "PAD_IDX = vocab2idx[PAD_TOKEN]\n",
    "BOS_IDX = vocab2idx[BOS_TOKEN]\n",
    "EOS_IDX = vocab2idx[EOS_TOKEN]\n",
    "VOCAB_SIZE = len(vocab_list)\n",
    "\n",
    "print(f\"The dictionary is formed. Size: {VOCAB_SIZE} symbols (including special tokens).\")\n",
    "\n",
    "\n",
    "\n",
    "def text_to_tensor(text: str, max_len=MAX_LEN) -> torch.Tensor:\n",
    "    tokens = [BOS_IDX]\n",
    "    for ch in text:\n",
    "        if ch in vocab2idx:\n",
    "            tokens.append(vocab2idx[ch])\n",
    "        else:\n",
    "            tokens.append(PAD_IDX)\n",
    "    tokens.append(EOS_IDX)\n",
    "\n",
    "    if len(tokens) < max_len:\n",
    "        tokens += [PAD_IDX] * (max_len - len(tokens))\n",
    "    else:\n",
    "        tokens = tokens[:max_len]\n",
    "\n",
    "    return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "def tensor_to_text(tensor: torch.Tensor) -> str:\n",
    "    chars = []\n",
    "    for idx in tensor:\n",
    "        idx_val = idx.item()\n",
    "        if idx_val == BOS_IDX:\n",
    "            continue\n",
    "        if idx_val == EOS_IDX or idx_val == PAD_IDX:\n",
    "            break\n",
    "        chars.append(idx2vocab[idx_val])\n",
    "    return \"\".join(chars)\n",
    "\n",
    "\n",
    "\n",
    "class TranslitDataset(Dataset):\n",
    "    def __init__(self, dataframe, max_len=MAX_LEN):\n",
    "        self.df = dataframe\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        mistake_word = row[\"MISTAKE\"]\n",
    "        correct_word = row[\"CORRECT\"]\n",
    "\n",
    "        src_tensor = text_to_tensor(mistake_word, self.max_len)\n",
    "        tgt_tensor = text_to_tensor(correct_word, self.max_len)\n",
    "        return src_tensor, tgt_tensor\n",
    "\n",
    "dataset = TranslitDataset(df, max_len=MAX_LEN)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.W1 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.W2 = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        decoder_hidden = decoder_hidden.unsqueeze(1)\n",
    "\n",
    "        energy = self.v(\n",
    "            torch.tanh(\n",
    "                self.W1(decoder_hidden) + self.W2(encoder_outputs)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        attention_weights = torch.softmax(energy.squeeze(2), dim=1)\n",
    "\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)\n",
    "\n",
    "        context = context.squeeze(1)\n",
    "        return context, attention_weights\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (h, c) = self.lstm(embedded)\n",
    "        return outputs, (h, c)\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_size + hidden_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, input_token, hidden, cell, encoder_outputs):\n",
    "        embedded = self.embedding(input_token)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "\n",
    "        context, attn_weights = self.attention(hidden, encoder_outputs)\n",
    "        context = context.unsqueeze(1)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, (h_new, c_new) = self.lstm(rnn_input, (hidden.unsqueeze(0), cell.unsqueeze(0)))\n",
    "\n",
    "        h_new = h_new.squeeze(0)\n",
    "        c_new = c_new.squeeze(0)\n",
    "\n",
    "        logits = self.fc_out(output.squeeze(1))\n",
    "\n",
    "        return logits, h_new, c_new, attn_weights\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderRNN(vocab_size, embed_size, hidden_size, pad_idx)\n",
    "        self.decoder = DecoderRNN(vocab_size, embed_size, hidden_size, pad_idx)\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "\n",
    "\n",
    "        encoder_outputs, (h, c) = self.encoder(src)\n",
    "        h = h.squeeze(0)\n",
    "        c = c.squeeze(0)\n",
    "\n",
    "        outputs = []\n",
    "        input_token = tgt[:, 0]\n",
    "        for t in range(1, tgt_len):\n",
    "            logits, h, c, _ = self.decoder(input_token, h, c, encoder_outputs)\n",
    "            outputs.append(logits.unsqueeze(1))\n",
    "\n",
    "            input_token = tgt[:, t]\n",
    "\n",
    "        logits_seq = torch.cat(outputs, dim=1)\n",
    "        return logits_seq\n",
    "\n",
    "\n",
    "EMBED_SIZE = 128\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "model = Seq2Seq(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_size=EMBED_SIZE,\n",
    "    hidden_size=HIDDEN_SIZE,\n",
    "    pad_idx=PAD_IDX\n",
    ").to(DEVICE)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, loss_fn, device=DEVICE):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        logits_seq = model(src, tgt)\n",
    "\n",
    "        tgt_y = tgt[:, 1:]\n",
    "\n",
    "        logits_seq = logits_seq.reshape(-1, VOCAB_SIZE)\n",
    "        tgt_y = tgt_y.reshape(-1)\n",
    "\n",
    "        loss = loss_fn(logits_seq, tgt_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    avg_loss = train_one_epoch(model, dataloader, optimizer, loss_fn, device=DEVICE)\n",
    "    print(f\"[Epoch {epoch+1}/{MAX_EPOCHS}] loss = {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "save_data = {\n",
    "    \"model_state_dict\": model.state_dict(),\n",
    "    \"vocab2idx\": vocab2idx,\n",
    "    \"idx2vocab\": idx2vocab,\n",
    "    \"pad_idx\": PAD_IDX,\n",
    "    \"bos_idx\": BOS_IDX,\n",
    "    \"eos_idx\": EOS_IDX,\n",
    "    \"embed_size\": EMBED_SIZE,\n",
    "    \"hidden_size\": HIDDEN_SIZE,\n",
    "    \"max_len\": MAX_LEN\n",
    "}\n",
    "torch.save(save_data, MODEL_SAVE_PATH)\n",
    "print(f\"The model and dictionary are saved in {MODEL_SAVE_PATH}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
